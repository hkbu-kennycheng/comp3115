# Lab 6: Linear Regression with Orange

## Introduction to Linear Regression

Linear regression is a supervised learning algorithm that is used to predict the value of a continuous variable. Linear regression is a simple algorithm that is easy to understand and implement. Linear regression is often used as a baseline algorithm to compare the performance of more complex algorithms.

## Generating synthetic data for experiments

Given a data set with pairs of input feature vector and output value(s), a regression model can be learned. There are different regression models that you can try, including linear regression, ridge Regression, etc.

Let us consider the simplest form of a linear regression model (i.e., fitting a straight line to one-dimensional input data). This simple model can be extended to model more complicated data behavior.

We will start with the most familiar linear regression, a straight-line fit to data. A straight-line fit is a model of the form $y = θ1x + θ0$ where $θ1$ is commonly known as the slope, and 
$θ0$ is commonly known as the intercept (also known as bias).

Consider the following data which is generated by scattering about a line with a slope of 2 and an intercept of -5 with some random fluctuations. We will use Orange to generate synthetic data for experiments. Orange provides a number of data generators that can be used to generate synthetic data for experiments. The following code snippet shows how to generate a dataset with 100 instances and 2 features using the `Python Script` widget:

```python
import numpy as np
from Orange.data import Table, Domain, ContinuousVariable, DiscreteVariable

domain = Domain([ContinuousVariable("x"),
                 ContinuousVariable("y")])

rng = np.random.RandomState(1)
x = 10 * rng.rand(50)          # randomly generated input x
y = 2 * x - 5 + rng.randn(50)  # use theta_1*x + theta_0 + noise to generated output y

arr = list(zip(x, y))
out_data = Table.from_numpy(domain, arr)
```

The code above creates a table with two columns: `x` and `y`.